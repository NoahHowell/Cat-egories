{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afc66935",
   "metadata": {},
   "source": [
    "# Cat-egories YouTube Channel Data Scraper\n",
    "\n",
    "This notebook uses the YouTube Data API v3 to collect metadata from cat-themed YouTube channels.\n",
    "\n",
    "## What it does:\n",
    "- Reads channel IDs from `accounts.txt`\n",
    "- Fetches channel metadata (subscribers, total views, etc.)\n",
    "- Retrieves video data including:\n",
    "  - Titles, descriptions, tags, hashtags\n",
    "  - View counts, likes, comments\n",
    "  - Published dates\n",
    "- Exports data to separate CSV files for each channel\n",
    "- Creates a summary CSV with all channels\n",
    "\n",
    "## Before running:\n",
    "1. **Get a YouTube API Key:**\n",
    "   - Go to [Google Cloud Console](https://console.cloud.google.com/)\n",
    "   - Create a new project or select existing\n",
    "   - Enable YouTube Data API v3\n",
    "   - Create credentials (API Key)\n",
    "\n",
    "2. **Add your API key to `.env` file:**\n",
    "   - Open the `.env` file in this directory\n",
    "   - Replace `your_api_key_here` with your actual YouTube API key\n",
    "   - Save the file\n",
    "\n",
    "3. **Add channel IDs** to `accounts.txt` (already populated with examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c008cf6",
   "metadata": {},
   "source": [
    "## What Data Gets Scraped?\n",
    "\n",
    "### For Each Channel:\n",
    "The scraper collects **channel-level metadata**:\n",
    "- **Channel Title** - Name of the channel\n",
    "- **Channel Description** - About section text\n",
    "- **Subscriber Count** - Number of subscribers\n",
    "- **Total View Count** - All-time views across all videos\n",
    "- **Video Count** - Total number of videos published\n",
    "- **Published Date** - When the channel was created\n",
    "\n",
    "### For Each Video (currently up to 50 per channel):\n",
    "The scraper collects **video-level data**:\n",
    "\n",
    "**Content Information:**\n",
    "- **Video Title** - The video's title\n",
    "- **Description** - Full video description text\n",
    "- **Tags** - YouTube tags the creator assigned (stored as pipe-separated: `tag1|tag2|tag3`)\n",
    "- **Hashtags** - Hashtags extracted from the description (stored as pipe-separated)\n",
    "- **Duration** - Video length (in ISO format)\n",
    "- **Published Date** - When the video was uploaded\n",
    "\n",
    "**Engagement Metrics:**\n",
    "- **View Count** - Number of views\n",
    "- **Like Count** - Number of likes\n",
    "- **Comment Count** - Number of comments\n",
    "- **Favorite Count** - Number of favorites (usually 0, legacy metric)\n",
    "\n",
    "**Identifiers:**\n",
    "- **Video ID** - Unique YouTube video identifier\n",
    "- **Channel ID** - Unique channel identifier\n",
    "- **Channel Title** - For easy reference\n",
    "\n",
    "### Example Row from CSV:\n",
    "```\n",
    "video_id: dQw4w9WgXcQ\n",
    "title: Cute Cat Playing with Box\n",
    "description: My cat loves this box! #catsofyoutube #funny\n",
    "tags: cat|funny|pets|animals\n",
    "hashtags: #catsofyoutube|#funny\n",
    "view_count: 125000\n",
    "like_count: 3500\n",
    "comment_count: 450\n",
    "duration: PT5M32S\n",
    "published_at: 2024-03-15T10:30:00Z\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f989350",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab83809",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install google-api-python-client\n",
    "%pip install pandas tqdm python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4ba934",
   "metadata": {},
   "source": [
    "## 2. Import Libraries, API Config, and Print Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9eac16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from googleapiclient.discovery import build\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Color codes for terminal output\n",
    "class Colors:\n",
    "    GREEN = '\\033[92m'\n",
    "    RED = '\\033[91m'\n",
    "    YELLOW = '\\033[93m'\n",
    "    BLUE = '\\033[94m'\n",
    "    RESET = '\\033[0m'\n",
    "    BOLD = '\\033[1m'\n",
    "\n",
    "def print_success(message):\n",
    "    \"\"\"Print success message in green\"\"\"\n",
    "    print(f\"{Colors.GREEN}{message}{Colors.RESET}\")\n",
    "\n",
    "def print_error(message):\n",
    "    \"\"\"Print error message in red\"\"\"\n",
    "    print(f\"{Colors.RED}{message}{Colors.RESET}\")\n",
    "\n",
    "def print_warning(message):\n",
    "    \"\"\"Print warning message in yellow\"\"\"\n",
    "    print(f\"{Colors.YELLOW}{message}{Colors.RESET}\")\n",
    "\n",
    "def print_info(message):\n",
    "    \"\"\"Print info message in blue\"\"\"\n",
    "    print(f\"{Colors.BLUE}{message}{Colors.RESET}\")\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get YouTube API Key from environment variable\n",
    "API_KEY = os.getenv('YOUTUBE_API_KEY')\n",
    "\n",
    "if not API_KEY or API_KEY == 'your_api_key_here':\n",
    "    raise ValueError(\"Please set your YOUTUBE_API_KEY in the .env file\")\n",
    "\n",
    "# Initialize YouTube API client\n",
    "youtube = build('youtube', 'v3', developerKey=API_KEY)\n",
    "print_success(\"YouTube API client initialized successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83acde9e",
   "metadata": {},
   "source": [
    "## 3. Define Helper Functions\n",
    "\n",
    "These functions handle:\n",
    "- Reading channel IDs from file\n",
    "- Fetching channel information\n",
    "- Retrieving video lists\n",
    "- Getting detailed video metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af93d99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolve_channel_identifier(youtube, identifier):\n",
    "    \"\"\"\n",
    "    Resolve a channel identifier to a channel ID.\n",
    "    Handles @username, custom URLs, and direct channel IDs.\n",
    "    \"\"\"\n",
    "    identifier = identifier.strip()\n",
    "    \n",
    "    # If it's already a channel ID (starts with UC), return it\n",
    "    if identifier.startswith('UC') and len(identifier) == 24:\n",
    "        return identifier\n",
    "    \n",
    "    # If it starts with @, it's a username handle\n",
    "    if identifier.startswith('@'):\n",
    "        username = identifier[1:]  # Remove the @\n",
    "        try:\n",
    "            request = youtube.channels().list(\n",
    "                part='id',\n",
    "                forHandle=username\n",
    "            )\n",
    "            response = request.execute()\n",
    "            if response.get('items'):\n",
    "                return response['items'][0]['id']\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Try as custom URL or username\n",
    "    try:\n",
    "        request = youtube.channels().list(\n",
    "            part='id',\n",
    "            forUsername=identifier.replace('@', '')\n",
    "        )\n",
    "        response = request.execute()\n",
    "        if response.get('items'):\n",
    "            return response['items'][0]['id']\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    print_error(f\"Could not resolve: {identifier}\")\n",
    "    return None\n",
    "\n",
    "\n",
    "def read_channel_ids(filename='accounts.txt'):\n",
    "    \"\"\"\n",
    "    Read channel identifiers from a text file and resolve them to channel IDs.\n",
    "    Skips empty lines and lines starting with #\n",
    "    \"\"\"\n",
    "    identifiers = []\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line and not line.startswith('#'):\n",
    "                identifiers.append(line)\n",
    "    return identifiers\n",
    "\n",
    "# Test the function\n",
    "identifiers = read_channel_ids('accounts.txt')\n",
    "print_info(f\"Found {len(identifiers)} channel identifiers:\")\n",
    "for identifier in identifiers:\n",
    "    print(f\"  - {identifier}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cfe46434",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_channel_info(youtube, channel_id):\n",
    "    \"\"\"\n",
    "    Fetch channel metadata including title, description, subscriber count, view count, etc.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        request = youtube.channels().list(\n",
    "            part='snippet,statistics,contentDetails',\n",
    "            id=channel_id\n",
    "        )\n",
    "        response = request.execute()\n",
    "        \n",
    "        if not response.get('items'):\n",
    "            print(f\"No channel found for ID: {channel_id}\")\n",
    "            return None\n",
    "        \n",
    "        channel = response['items'][0]\n",
    "        \n",
    "        # Clean description for CSV compatibility\n",
    "        description = channel['snippet']['description']\n",
    "        description_clean = description.replace('\\n', ' ').replace('\\r', ' ')\n",
    "        \n",
    "        channel_data = {\n",
    "            'channel_id': channel_id,\n",
    "            'channel_title': channel['snippet']['title'],\n",
    "            'channel_description': description_clean,  # Cleaned description\n",
    "            'published_at': channel['snippet']['publishedAt'],\n",
    "            'subscriber_count': channel['statistics'].get('subscriberCount', 0),\n",
    "            'view_count': channel['statistics'].get('viewCount', 0),\n",
    "            'video_count': channel['statistics'].get('videoCount', 0),\n",
    "            'uploads_playlist_id': channel['contentDetails']['relatedPlaylists']['uploads']\n",
    "        }\n",
    "        \n",
    "        return channel_data\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching channel info for {channel_id}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f9f5263",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_channel_videos(youtube, uploads_playlist_id, max_results=50):\n",
    "    \"\"\"\n",
    "    Fetch video IDs from a channel's uploads playlist.\n",
    "    Returns a list of video IDs.\n",
    "    \"\"\"\n",
    "    video_ids = []\n",
    "    next_page_token = None\n",
    "    \n",
    "    try:\n",
    "        while len(video_ids) < max_results:\n",
    "            request = youtube.playlistItems().list(\n",
    "                part='contentDetails',\n",
    "                playlistId=uploads_playlist_id,\n",
    "                # Can be updated to fetch more than 50 if needed, should check API limits\n",
    "                maxResults=min(50, max_results - len(video_ids)),\n",
    "                pageToken=next_page_token\n",
    "            )\n",
    "            response = request.execute()\n",
    "            \n",
    "            for item in response['items']:\n",
    "                video_ids.append(item['contentDetails']['videoId'])\n",
    "            \n",
    "            next_page_token = response.get('nextPageToken')\n",
    "            \n",
    "            if not next_page_token:\n",
    "                break\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching videos: {e}\")\n",
    "    \n",
    "    return video_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b12174ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_details(youtube, video_ids):\n",
    "    \"\"\"\n",
    "    Fetch detailed information for a list of video IDs.\n",
    "    Includes title, description, tags, views, likes, comments, etc.\n",
    "    NOTE: Does NOT include channel-level data - that goes in the summary CSV.\n",
    "    \"\"\"\n",
    "    all_video_data = []\n",
    "    \n",
    "    # YouTube API allows max 50 videos per request\n",
    "    for i in range(0, len(video_ids), 50):\n",
    "        batch = video_ids[i:i+50]\n",
    "        \n",
    "        try:\n",
    "            request = youtube.videos().list(\n",
    "                part='snippet,statistics,contentDetails',\n",
    "                id=','.join(batch)\n",
    "            )\n",
    "            response = request.execute()\n",
    "            \n",
    "            for video in response['items']:\n",
    "                # Extract hashtags from description\n",
    "                description = video['snippet'].get('description', '')\n",
    "                # Replace newlines with spaces to prevent CSV issues\n",
    "                description_clean = description.replace('\\n', ' ').replace('\\r', ' ')\n",
    "                \n",
    "                hashtags = [word for word in description.split() if word.startswith('#')]\n",
    "                \n",
    "                video_data = {\n",
    "                    'video_id': video['id'],\n",
    "                    'title': video['snippet']['title'],\n",
    "                    'description': description_clean,  # Cleaned description\n",
    "                    'published_at': video['snippet']['publishedAt'],\n",
    "                    'tags': '|'.join(video['snippet'].get('tags', [])),  # Join tags with |\n",
    "                    'hashtags': '|'.join(hashtags),  # Join hashtags with |\n",
    "                    'duration': video['contentDetails']['duration'],\n",
    "                    'view_count': video['statistics'].get('viewCount', 0),\n",
    "                    'like_count': video['statistics'].get('likeCount', 0),\n",
    "                    'comment_count': video['statistics'].get('commentCount', 0),\n",
    "                }\n",
    "                \n",
    "                all_video_data.append(video_data)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching video details: {e}\")\n",
    "    \n",
    "    return all_video_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "571a15fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_channel_data(youtube, channel_id, max_videos=50):\n",
    "    \"\"\"\n",
    "    Main function to scrape all data for a single channel.\n",
    "    Returns a DataFrame with video details (NO channel info - that's in summary).\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Scraping channel: {channel_id}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Get channel info\n",
    "    channel_info = get_channel_info(youtube, channel_id)\n",
    "    if not channel_info:\n",
    "        return None, None\n",
    "    \n",
    "    print(f\"Channel: {channel_info['channel_title']}\")\n",
    "    print(f\"Subscribers: {channel_info['subscriber_count']}\")\n",
    "    print(f\"Total Views: {channel_info['view_count']}\")\n",
    "    print(f\"Total Videos: {channel_info['video_count']}\")\n",
    "    \n",
    "    # Get video IDs\n",
    "    print(f\"\\nFetching up to {max_videos} videos...\")\n",
    "    video_ids = get_channel_videos(youtube, channel_info['uploads_playlist_id'], max_videos)\n",
    "    print(f\"Found {len(video_ids)} videos\")\n",
    "    \n",
    "    # Get video details\n",
    "    print(\"Fetching video details...\")\n",
    "    video_data = get_video_details(youtube, video_ids)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(video_data)\n",
    "    \n",
    "    # Convert numeric columns\n",
    "    numeric_cols = ['view_count', 'like_count', 'comment_count']\n",
    "    for col in numeric_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0).astype(int)\n",
    "    \n",
    "    print(f\"Successfully scraped {len(df)} videos\")\n",
    "    \n",
    "    return df, channel_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1078262",
   "metadata": {},
   "source": [
    "## Main Scraping Process\n",
    "- Scrape all the channels from `accounts.txt` and save the data:\n",
    "\n",
    "### File Structure:\n",
    " - **Individual channel CSVs**: `{ChannelName}.csv` \n",
    "   - Contains ONLY video data (no channel info)\n",
    " - **Summary CSV**: `channels_summary.csv`\n",
    "   - Contains one row per channel with channel metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ead240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data directory if it doesn't exist\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "# Read channel identifiers\n",
    "identifiers = read_channel_ids('accounts.txt')\n",
    "\n",
    "print_info(\"Resolving channel identifiers...\")\n",
    "channel_ids = []\n",
    "for identifier in identifiers:\n",
    "    channel_id = resolve_channel_identifier(youtube, identifier)\n",
    "    if channel_id:\n",
    "        print_success(f\"  [SUCCESS] {identifier} -> {channel_id}\")\n",
    "        channel_ids.append(channel_id)\n",
    "    else:\n",
    "        print_error(f\"  [FAILED] {identifier} -> Could not resolve\")\n",
    "\n",
    "print_info(f\"\\nResolved {len(channel_ids)} out of {len(identifiers)} channels\\n\")\n",
    "\n",
    "# Store all results\n",
    "all_channels_data = []\n",
    "channel_metadata = []\n",
    "\n",
    "# Scrape each channel\n",
    "for channel_id in tqdm(channel_ids, desc=\"Scraping channels\"):\n",
    "    df, channel_info = scrape_channel_data(youtube, channel_id, max_videos=50)\n",
    "    \n",
    "    if df is not None and channel_info is not None:\n",
    "        channel_name = channel_info['channel_title']\n",
    "        # Remove special characters and use only ASCII-safe characters\n",
    "        clean_name = ''.join(c for c in channel_name if c.isalnum() or c in (' ', '-', '_'))\n",
    "        clean_name = clean_name.replace(' ', '_').strip('_')\n",
    "        filename = f\"data/{clean_name}.csv\"\n",
    "        \n",
    "        df.to_csv(filename, index=False, encoding='utf-8')\n",
    "        print_success(f\"[SAVED] {filename}\\n\")\n",
    "        \n",
    "        all_channels_data.append(df)\n",
    "        channel_metadata.append(channel_info)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print_success(f\"Scraping Complete!\")\n",
    "print_info(f\"Successfully scraped {len(all_channels_data)} channels\")\n",
    "print_info(f\"CSV files saved in the 'data/' directory\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c925f93",
   "metadata": {},
   "source": [
    "## Summary CSV Creation\n",
    "- After scraping each channel, append its metadata to a summary list\n",
    "- At the end, convert this list to a DataFrame and save as `channels_summary.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d33e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary DataFrame from channel metadata\n",
    "summary_df = pd.DataFrame(channel_metadata)\n",
    "\n",
    "# Calculate average engagement per channel from video data\n",
    "# Match up with channel metadata by index (they're in the same order)\n",
    "engagement_stats = []\n",
    "for i, df in enumerate(all_channels_data):\n",
    "    if len(df) > 0:\n",
    "        # Get the corresponding channel info\n",
    "        channel_info = channel_metadata[i]\n",
    "        \n",
    "        stats = {\n",
    "            'channel_id': channel_info['channel_id'],\n",
    "            'channel_title': channel_info['channel_title'],\n",
    "            'total_videos_scraped': len(df),\n",
    "            'avg_views': df['view_count'].mean(),\n",
    "            'avg_likes': df['like_count'].mean(),\n",
    "            'avg_comments': df['comment_count'].mean(),\n",
    "            'total_views_scraped_videos': df['view_count'].sum(),\n",
    "            'total_likes_scraped_videos': df['like_count'].sum(),\n",
    "        }\n",
    "        engagement_stats.append(stats)\n",
    "\n",
    "engagement_df = pd.DataFrame(engagement_stats)\n",
    "\n",
    "# Merge with channel metadata\n",
    "if len(engagement_df) > 0:\n",
    "    summary_full = pd.merge(summary_df, engagement_df, on=['channel_id', 'channel_title'], how='left')\n",
    "    \n",
    "    # Save summary\n",
    "    summary_full.to_csv('data/channels_summary.csv', index=False)\n",
    "    print(\"Channel Summary:\")\n",
    "    print(summary_full[['channel_title', 'subscriber_count', 'video_count', \n",
    "                        'total_videos_scraped', 'avg_views', 'avg_likes']].to_string(index=False))\n",
    "    print(f\"\\nSummary saved to: data/channels_summary.csv\")\n",
    "else:\n",
    "    print(\"No data scraped\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
