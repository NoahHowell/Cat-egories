{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afc66935",
   "metadata": {},
   "source": [
    "# Cat-egories YouTube Channel Data Scraper\n",
    "\n",
    "This notebook uses the YouTube Data API v3 to collect metadata from cat-themed YouTube channels.\n",
    "\n",
    "## What it does:\n",
    "- Reads channel IDs from `accounts.txt`\n",
    "- Fetches channel metadata (subscribers, total views, etc.)\n",
    "- Retrieves video data including:\n",
    "  - Titles, descriptions, tags, hashtags\n",
    "  - View counts, likes, comments\n",
    "  - Published dates\n",
    "- Exports data to separate CSV files for each channel\n",
    "- Creates a summary CSV with all channels\n",
    "\n",
    "## Before running:\n",
    "1. **Get a YouTube API Key:**\n",
    "   - Go to [Google Cloud Console](https://console.cloud.google.com/)\n",
    "   - Create a new project or select existing\n",
    "   - Enable YouTube Data API v3\n",
    "   - Create credentials (API Key)\n",
    "\n",
    "2. **Add your API key to `.env` file:**\n",
    "   - Open the `.env` file in this directory\n",
    "   - Replace `your_api_key_here` with your actual YouTube API key\n",
    "   - Save the file\n",
    "\n",
    "3. **Add channel IDs** to `accounts.txt` (already populated with examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c008cf6",
   "metadata": {},
   "source": [
    "## What Data Gets Scraped?\n",
    "\n",
    "### For Each Channel:\n",
    "The scraper collects **channel-level metadata**:\n",
    "- **Channel Title** - Name of the channel\n",
    "- **Channel Description** - About section text\n",
    "- **Subscriber Count** - Number of subscribers\n",
    "- **Total View Count** - All-time views across all videos\n",
    "- **Video Count** - Total number of videos published\n",
    "- **Published Date** - When the channel was created\n",
    "\n",
    "### For Each Video (currently up to 50 per channel):\n",
    "The scraper collects **video-level data**:\n",
    "\n",
    "**Content Information:**\n",
    "- **Video Title** - The video's title\n",
    "- **Description** - Full video description text\n",
    "- **Tags** - YouTube tags the creator assigned (stored as pipe-separated: `tag1|tag2|tag3`)\n",
    "- **Hashtags** - Hashtags extracted from the description (stored as pipe-separated)\n",
    "- **Duration** - Video length (in ISO format)\n",
    "- **Published Date** - When the video was uploaded\n",
    "\n",
    "**Engagement Metrics:**\n",
    "- **View Count** - Number of views\n",
    "- **Like Count** - Number of likes\n",
    "- **Comment Count** - Number of comments\n",
    "- **Favorite Count** - Number of favorites (usually 0, legacy metric)\n",
    "\n",
    "**Identifiers:**\n",
    "- **Video ID** - Unique YouTube video identifier\n",
    "- **Channel ID** - Unique channel identifier\n",
    "- **Channel Title** - For easy reference\n",
    "\n",
    "### Example Row from CSV:\n",
    "```\n",
    "video_id: dQw4w9WgXcQ\n",
    "title: Cute Cat Playing with Box\n",
    "description: My cat loves this box! #catsofyoutube #funny\n",
    "tags: cat|funny|pets|animals\n",
    "hashtags: #catsofyoutube|#funny\n",
    "view_count: 125000\n",
    "like_count: 3500\n",
    "comment_count: 450\n",
    "duration: PT5M32S\n",
    "published_at: 2024-03-15T10:30:00Z\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f989350",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ab83809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: google-api-python-client in c:\\users\\duber\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (2.187.0)\n",
      "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in c:\\users\\duber\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from google-api-python-client) (0.31.0)\n",
      "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0 in c:\\users\\duber\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from google-api-python-client) (2.43.0)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in c:\\users\\duber\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from google-api-python-client) (0.2.1)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5 in c:\\users\\duber\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from google-api-python-client) (2.28.1)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in c:\\users\\duber\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from google-api-python-client) (4.2.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in c:\\users\\duber\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (1.72.0)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 in c:\\users\\duber\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (6.32.1)\n",
      "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in c:\\users\\duber\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (1.26.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in c:\\users\\duber\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (2.32.5)\n",
      "Requirement already satisfied: cachetools<7.0,>=2.0.0 in c:\\users\\duber\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client) (6.2.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\duber\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\duber\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client) (4.9.1)\n",
      "Requirement already satisfied: pyparsing<4,>=3.0.4 in c:\\users\\duber\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client) (3.1.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\duber\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\duber\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\duber\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\duber\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (2025.11.12)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\users\\duber\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from rsa<5,>=3.1.4->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client) (0.6.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ (C:\\Users\\duber\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ (C:\\Users\\duber\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ (C:\\Users\\duber\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in c:\\users\\duber\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (2.3.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\duber\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (4.67.1)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\duber\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (1.2.1)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\duber\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pandas) (2.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\duber\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\duber\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\duber\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\duber\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\duber\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ (C:\\Users\\duber\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ (C:\\Users\\duber\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ (C:\\Users\\duber\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "%pip install google-api-python-client\n",
    "%pip install pandas tqdm python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4ba934",
   "metadata": {},
   "source": [
    "## 2. Import Libraries, API Config, and Print Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a9eac16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mYouTube API client initialized successfully\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "from googleapiclient.discovery import build\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Color codes for terminal output\n",
    "class Colors:\n",
    "    GREEN = '\\033[92m'\n",
    "    RED = '\\033[91m'\n",
    "    YELLOW = '\\033[93m'\n",
    "    BLUE = '\\033[94m'\n",
    "    RESET = '\\033[0m'\n",
    "    BOLD = '\\033[1m'\n",
    "\n",
    "def print_success(message):\n",
    "    \"\"\"Print success message in green\"\"\"\n",
    "    print(f\"{Colors.GREEN}{message}{Colors.RESET}\")\n",
    "\n",
    "def print_error(message):\n",
    "    \"\"\"Print error message in red\"\"\"\n",
    "    print(f\"{Colors.RED}{message}{Colors.RESET}\")\n",
    "\n",
    "def print_warning(message):\n",
    "    \"\"\"Print warning message in yellow\"\"\"\n",
    "    print(f\"{Colors.YELLOW}{message}{Colors.RESET}\")\n",
    "\n",
    "def print_info(message):\n",
    "    \"\"\"Print info message in blue\"\"\"\n",
    "    print(f\"{Colors.BLUE}{message}{Colors.RESET}\")\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get YouTube API Key from environment variable\n",
    "API_KEY = os.getenv('YOUTUBE_API_KEY')\n",
    "\n",
    "if not API_KEY or API_KEY == 'your_api_key_here':\n",
    "    raise ValueError(\"Please set your YOUTUBE_API_KEY in the .env file\")\n",
    "\n",
    "# Initialize YouTube API client\n",
    "youtube = build('youtube', 'v3', developerKey=API_KEY)\n",
    "print_success(\"YouTube API client initialized successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83acde9e",
   "metadata": {},
   "source": [
    "## 3. Define Helper Functions\n",
    "\n",
    "These functions handle:\n",
    "- Reading channel IDs from file\n",
    "- Fetching channel information\n",
    "- Retrieving video lists\n",
    "- Getting detailed video metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af93d99f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94mFound 29 channel identifiers:\u001b[0m\n",
      "  - @CrunchycatLuna\n",
      "  - @chacha-rme\n",
      "  - @funnycattshorts\n",
      "  - @PrincessNikacat\n",
      "  - @ChipTheManx\n",
      "  - @cats101_03\n",
      "  - @CatPusicTeam\n",
      "  - @OwlKitty\n",
      "  - @coleandmarmalade\n",
      "  - @DailyDoseOfInternetCats\n",
      "  - @ChefCatChangAn666\n",
      "  - @FeedingStreetCats\n",
      "  - @TastyPaws\n",
      "  - @cat.mp4.666\n",
      "  - @LittleLove666\n",
      "  - @PawsomeCatsOfTheInternet\n",
      "  - @PurrfectPets24\n",
      "  - @CaDAnimals\n",
      "  - @dextheorangecat\n",
      "  - @RenusDelph\n",
      "  - @FunnyandcuteCatLife\n",
      "  - @Maine_Coon_Kittens\n",
      "  - @walterthecatt\n",
      "  - @CatManChrisPoole\n",
      "  - @TakeYourDoseOfCats\n",
      "  - @funcatflicks-l\n",
      "  - @funnyPaws_show\n",
      "  - @Meowphorius\n",
      "  - @elcatogato\n"
     ]
    }
   ],
   "source": [
    "def resolve_channel_identifier(youtube, identifier):\n",
    "    \"\"\"\n",
    "    Resolve a channel identifier to a channel ID.\n",
    "    Handles @username, custom URLs, and direct channel IDs.\n",
    "    \"\"\"\n",
    "    identifier = identifier.strip()\n",
    "    \n",
    "    # If it's already a channel ID (starts with UC), return it\n",
    "    if identifier.startswith('UC') and len(identifier) == 24:\n",
    "        return identifier\n",
    "    \n",
    "    # If it starts with @, it's a username handle\n",
    "    if identifier.startswith('@'):\n",
    "        username = identifier[1:]  # Remove the @\n",
    "        try:\n",
    "            request = youtube.channels().list(\n",
    "                part='id',\n",
    "                forHandle=username\n",
    "            )\n",
    "            response = request.execute()\n",
    "            if response.get('items'):\n",
    "                return response['items'][0]['id']\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Try as custom URL or username\n",
    "    try:\n",
    "        request = youtube.channels().list(\n",
    "            part='id',\n",
    "            forUsername=identifier.replace('@', '')\n",
    "        )\n",
    "        response = request.execute()\n",
    "        if response.get('items'):\n",
    "            return response['items'][0]['id']\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    print_error(f\"Could not resolve: {identifier}\")\n",
    "    return None\n",
    "\n",
    "\n",
    "def read_channel_ids(filename='accounts.txt'):\n",
    "    \"\"\"\n",
    "    Read channel identifiers from a text file and resolve them to channel IDs.\n",
    "    Skips empty lines and lines starting with #\n",
    "    \"\"\"\n",
    "    identifiers = []\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line and not line.startswith('#'):\n",
    "                identifiers.append(line)\n",
    "    return identifiers\n",
    "\n",
    "# Test the function\n",
    "identifiers = read_channel_ids('accounts.txt')\n",
    "print_info(f\"Found {len(identifiers)} channel identifiers:\")\n",
    "for identifier in identifiers:\n",
    "    print(f\"  - {identifier}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cfe46434",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_channel_info(youtube, channel_id):\n",
    "    \"\"\"\n",
    "    Fetch channel metadata including title, description, subscriber count, view count, etc.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        request = youtube.channels().list(\n",
    "            part='snippet,statistics,contentDetails',\n",
    "            id=channel_id\n",
    "        )\n",
    "        response = request.execute()\n",
    "        \n",
    "        if not response.get('items'):\n",
    "            print(f\"No channel found for ID: {channel_id}\")\n",
    "            return None\n",
    "        \n",
    "        channel = response['items'][0]\n",
    "        \n",
    "        # Clean description for CSV compatibility\n",
    "        description = channel['snippet']['description']\n",
    "        description_clean = description.replace('\\n', ' ').replace('\\r', ' ')\n",
    "        \n",
    "        channel_data = {\n",
    "            'channel_id': channel_id,\n",
    "            'channel_title': channel['snippet']['title'],\n",
    "            'channel_description': description_clean,  # Cleaned description\n",
    "            'published_at': channel['snippet']['publishedAt'],\n",
    "            'subscriber_count': channel['statistics'].get('subscriberCount', 0),\n",
    "            'view_count': channel['statistics'].get('viewCount', 0),\n",
    "            'video_count': channel['statistics'].get('videoCount', 0),\n",
    "            'uploads_playlist_id': channel['contentDetails']['relatedPlaylists']['uploads']\n",
    "        }\n",
    "        \n",
    "        return channel_data\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching channel info for {channel_id}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f9f5263",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_channel_videos(youtube, uploads_playlist_id, max_results=50):\n",
    "    \"\"\"\n",
    "    Fetch video IDs from a channel's uploads playlist.\n",
    "    Returns a list of video IDs.\n",
    "    \"\"\"\n",
    "    video_ids = []\n",
    "    next_page_token = None\n",
    "    \n",
    "    try:\n",
    "        while len(video_ids) < max_results:\n",
    "            request = youtube.playlistItems().list(\n",
    "                part='contentDetails',\n",
    "                playlistId=uploads_playlist_id,\n",
    "                # Can be updated to fetch more than 50 if needed, should check API limits\n",
    "                maxResults=min(50, max_results - len(video_ids)),\n",
    "                pageToken=next_page_token\n",
    "            )\n",
    "            response = request.execute()\n",
    "            \n",
    "            for item in response['items']:\n",
    "                video_ids.append(item['contentDetails']['videoId'])\n",
    "            \n",
    "            next_page_token = response.get('nextPageToken')\n",
    "            \n",
    "            if not next_page_token:\n",
    "                break\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching videos: {e}\")\n",
    "    \n",
    "    return video_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b12174ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_details(youtube, video_ids):\n",
    "    \"\"\"\n",
    "    Fetch detailed information for a list of video IDs.\n",
    "    Includes title, description, tags, views, likes, comments, etc.\n",
    "    NOTE: Does NOT include channel-level data - that goes in the summary CSV.\n",
    "    \"\"\"\n",
    "    all_video_data = []\n",
    "    \n",
    "    # YouTube API allows max 50 videos per request\n",
    "    for i in range(0, len(video_ids), 50):\n",
    "        batch = video_ids[i:i+50]\n",
    "        \n",
    "        try:\n",
    "            request = youtube.videos().list(\n",
    "                part='snippet,statistics,contentDetails',\n",
    "                id=','.join(batch)\n",
    "            )\n",
    "            response = request.execute()\n",
    "            \n",
    "            for video in response['items']:\n",
    "                # Extract hashtags from description\n",
    "                description = video['snippet'].get('description', '')\n",
    "                # Replace newlines with spaces to prevent CSV issues\n",
    "                description_clean = description.replace('\\n', ' ').replace('\\r', ' ')\n",
    "                \n",
    "                hashtags = [word for word in description.split() if word.startswith('#')]\n",
    "                \n",
    "                video_data = {\n",
    "                    'video_id': video['id'],\n",
    "                    'title': video['snippet']['title'],\n",
    "                    'description': description_clean,  # Cleaned description\n",
    "                    'published_at': video['snippet']['publishedAt'],\n",
    "                    'tags': '|'.join(video['snippet'].get('tags', [])),  # Join tags with |\n",
    "                    'hashtags': '|'.join(hashtags),  # Join hashtags with |\n",
    "                    'duration': video['contentDetails']['duration'],\n",
    "                    'view_count': video['statistics'].get('viewCount', 0),\n",
    "                    'like_count': video['statistics'].get('likeCount', 0),\n",
    "                    'comment_count': video['statistics'].get('commentCount', 0),\n",
    "                }\n",
    "                \n",
    "                all_video_data.append(video_data)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching video details: {e}\")\n",
    "    \n",
    "    return all_video_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "571a15fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_channel_data(youtube, channel_id, max_videos=50):\n",
    "    \"\"\"\n",
    "    Main function to scrape all data for a single channel.\n",
    "    Returns a DataFrame with video details (NO channel info - that's in summary).\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Scraping channel: {channel_id}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Get channel info\n",
    "    channel_info = get_channel_info(youtube, channel_id)\n",
    "    if not channel_info:\n",
    "        return None, None\n",
    "    \n",
    "    print(f\"Channel: {channel_info['channel_title']}\")\n",
    "    print(f\"Subscribers: {channel_info['subscriber_count']}\")\n",
    "    print(f\"Total Views: {channel_info['view_count']}\")\n",
    "    print(f\"Total Videos: {channel_info['video_count']}\")\n",
    "    \n",
    "    # Get video IDs\n",
    "    print(f\"\\nFetching up to {max_videos} videos...\")\n",
    "    video_ids = get_channel_videos(youtube, channel_info['uploads_playlist_id'], max_videos)\n",
    "    print(f\"Found {len(video_ids)} videos\")\n",
    "    \n",
    "    # Get video details\n",
    "    print(\"Fetching video details...\")\n",
    "    video_data = get_video_details(youtube, video_ids)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(video_data)\n",
    "    \n",
    "    # Convert numeric columns\n",
    "    numeric_cols = ['view_count', 'like_count', 'comment_count']\n",
    "    for col in numeric_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0).astype(int)\n",
    "    \n",
    "    print(f\"Successfully scraped {len(df)} videos\")\n",
    "    \n",
    "    return df, channel_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1078262",
   "metadata": {},
   "source": [
    "## Main Scraping Process\n",
    "- Scrape all the channels from `accounts.txt` and save the data:\n",
    "\n",
    "### File Structure:\n",
    " - **Individual channel CSVs**: `{ChannelName}.csv` \n",
    "   - Contains ONLY video data (no channel info)\n",
    " - **Summary CSV**: `channels_summary.csv`\n",
    "   - Contains one row per channel with channel metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e3ead240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94mResolving channel identifiers...\u001b[0m\n",
      "\u001b[92m  [SUCCESS] @CrunchycatLuna -> UCbGvv6m9qjuZF9_fcljgvzw\u001b[0m\n",
      "\u001b[92m  [SUCCESS] @chacha-rme -> UCTJh2CW0v9MvqXAEC2mpsSQ\u001b[0m\n",
      "\u001b[92m  [SUCCESS] @funnycattshorts -> UCbTaoonXlipQ-JsG7lhfKog\u001b[0m\n",
      "\u001b[92m  [SUCCESS] @PrincessNikacat -> UC0X1dp5Gkhos0QJpainsjUA\u001b[0m\n",
      "\u001b[92m  [SUCCESS] @ChipTheManx -> UC6s-MijPBrcnr55akiKarmQ\u001b[0m\n",
      "\u001b[92m  [SUCCESS] @cats101_03 -> UCbWNEXG8GoszgTmJgaPgvnQ\u001b[0m\n",
      "\u001b[92m  [SUCCESS] @CatPusicTeam -> UCyIqcxz-vR_o2GK4HWuZL8w\u001b[0m\n",
      "\u001b[92m  [SUCCESS] @OwlKitty -> UCpLQXR116cLVUa1LRY8KS4w\u001b[0m\n",
      "\u001b[92m  [SUCCESS] @coleandmarmalade -> UCvmijL-eepDVHYSJHDY3d6w\u001b[0m\n",
      "\u001b[92m  [SUCCESS] @DailyDoseOfInternetCats -> UCTIa8uo_aisNdqQpMf4wKTg\u001b[0m\n",
      "\u001b[92m  [SUCCESS] @ChefCatChangAn666 -> UCv5R_Lzpu-4PopdHiDaH7-A\u001b[0m\n",
      "\u001b[92m  [SUCCESS] @FeedingStreetCats -> UC4Eqt3jI2inoD4TIcvuctYQ\u001b[0m\n",
      "\u001b[92m  [SUCCESS] @TastyPaws -> UCfVwRiihN37PIcmtlLihj6w\u001b[0m\n",
      "\u001b[92m  [SUCCESS] @cat.mp4.666 -> UC-xSWdtNE9tSp9SyfPaNHkQ\u001b[0m\n",
      "\u001b[92m  [SUCCESS] @LittleLove666 -> UCuPLku1Zrk6HMr2S51yGkpQ\u001b[0m\n",
      "\u001b[92m  [SUCCESS] @PawsomeCatsOfTheInternet -> UCMkRHqr6MjsJ7VEhXJnSRcg\u001b[0m\n",
      "\u001b[92m  [SUCCESS] @PurrfectPets24 -> UCAUqPgpcjPk_JVv8aPYL-rw\u001b[0m\n",
      "\u001b[92m  [SUCCESS] @CaDAnimals -> UCU0nAvDjqfXo1fTA7xn527w\u001b[0m\n",
      "\u001b[92m  [SUCCESS] @dextheorangecat -> UCfm5SLKZDJqVluh-oo6q9Cw\u001b[0m\n",
      "\u001b[92m  [SUCCESS] @RenusDelph -> UCezWXy_EVsFpQWQ6yk6a4FQ\u001b[0m\n",
      "\u001b[92m  [SUCCESS] @FunnyandcuteCatLife -> UCW_X3-IRNdOl9vrZ-zGthlQ\u001b[0m\n",
      "\u001b[92m  [SUCCESS] @Maine_Coon_Kittens -> UCmCOoX7E0hybWf1h7UJ4mrg\u001b[0m\n",
      "\u001b[92m  [SUCCESS] @walterthecatt -> UChKwZfRAjbfuWXo1NhKqHgQ\u001b[0m\n",
      "\u001b[92m  [SUCCESS] @CatManChrisPoole -> UC6VzUf8LyXxOz2ZkQP8_uhw\u001b[0m\n",
      "\u001b[92m  [SUCCESS] @TakeYourDoseOfCats -> UCZTM8ZG29egxUmYJ_pL-iZQ\u001b[0m\n",
      "\u001b[92m  [SUCCESS] @funcatflicks-l -> UCgpxaYSBRT6Ocb0wAw9S2OQ\u001b[0m\n",
      "\u001b[92m  [SUCCESS] @funnyPaws_show -> UCzdQGuxoc-B9WrWQnMF4vmA\u001b[0m\n",
      "\u001b[92m  [SUCCESS] @Meowphorius -> UCmSu6aPS3LKmf-5MrTr3TRg\u001b[0m\n",
      "\u001b[92m  [SUCCESS] @elcatogato -> UCxZKZ-T6Res7lrwDP9i9iUA\u001b[0m\n",
      "\u001b[94m\n",
      "Resolved 29 out of 29 channels\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping channels:   0%|          | 0/29 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Scraping channel: UCbGvv6m9qjuZF9_fcljgvzw\n",
      "============================================================\n",
      "Channel: Crunchycat\n",
      "Subscribers: 589000\n",
      "Total Views: 115594347\n",
      "Total Videos: 560\n",
      "\n",
      "Fetching up to 50 videos...\n",
      "Found 50 videos\n",
      "Fetching video details...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping channels:   3%|▎         | 1/29 [00:00<00:13,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully scraped 50 videos\n",
      "\u001b[92m[SAVED] data/Crunchycat.csv\n",
      "\u001b[0m\n",
      "\n",
      "============================================================\n",
      "Scraping channel: UCTJh2CW0v9MvqXAEC2mpsSQ\n",
      "============================================================\n",
      "Channel: 元野良猫チャチャとR me\n",
      "Subscribers: 663000\n",
      "Total Views: 328444839\n",
      "Total Videos: 549\n",
      "\n",
      "Fetching up to 50 videos...\n",
      "Found 50 videos\n",
      "Fetching video details...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping channels:   7%|▋         | 2/29 [00:01<00:14,  1.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully scraped 50 videos\n",
      "\u001b[92m[SAVED] data/元野良猫チャチャとR_me.csv\n",
      "\u001b[0m\n",
      "\n",
      "============================================================\n",
      "Scraping channel: UCbTaoonXlipQ-JsG7lhfKog\n",
      "============================================================\n",
      "Channel: The Meow Show\n",
      "Subscribers: 415000\n",
      "Total Views: 205245352\n",
      "Total Videos: 91\n",
      "\n",
      "Fetching up to 50 videos...\n",
      "Found 50 videos\n",
      "Fetching video details...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping channels:  10%|█         | 3/29 [00:01<00:14,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully scraped 50 videos\n",
      "\u001b[92m[SAVED] data/The_Meow_Show.csv\n",
      "\u001b[0m\n",
      "\n",
      "============================================================\n",
      "Scraping channel: UC0X1dp5Gkhos0QJpainsjUA\n",
      "============================================================\n",
      "Channel: Princess Nika cat\n",
      "Subscribers: 10800000\n",
      "Total Views: 6278111788\n",
      "Total Videos: 80\n",
      "\n",
      "Fetching up to 50 videos...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping channels:  14%|█▍        | 4/29 [00:02<00:12,  2.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 50 videos\n",
      "Fetching video details...\n",
      "Successfully scraped 50 videos\n",
      "\u001b[92m[SAVED] data/Princess_Nika_cat.csv\n",
      "\u001b[0m\n",
      "\n",
      "============================================================\n",
      "Scraping channel: UC6s-MijPBrcnr55akiKarmQ\n",
      "============================================================\n",
      "Channel: Chip The Manx\n",
      "Subscribers: 484000\n",
      "Total Views: 373055692\n",
      "Total Videos: 1122\n",
      "\n",
      "Fetching up to 50 videos...\n",
      "Found 50 videos\n",
      "Fetching video details...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping channels:  17%|█▋        | 5/29 [00:02<00:13,  1.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully scraped 50 videos\n",
      "\u001b[92m[SAVED] data/Chip_The_Manx.csv\n",
      "\u001b[0m\n",
      "\n",
      "============================================================\n",
      "Scraping channel: UCbWNEXG8GoszgTmJgaPgvnQ\n",
      "============================================================\n",
      "Channel: cats101\n",
      "Subscribers: 53200\n",
      "Total Views: 51075766\n",
      "Total Videos: 203\n",
      "\n",
      "Fetching up to 50 videos...\n",
      "Found 50 videos\n",
      "Fetching video details...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping channels:  21%|██        | 6/29 [00:03<00:11,  1.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully scraped 50 videos\n",
      "\u001b[92m[SAVED] data/cats101.csv\n",
      "\u001b[0m\n",
      "\n",
      "============================================================\n",
      "Scraping channel: UCyIqcxz-vR_o2GK4HWuZL8w\n",
      "============================================================\n",
      "Channel: CatPusic Team\n",
      "Subscribers: 1970000\n",
      "Total Views: 828472191\n",
      "Total Videos: 320\n",
      "\n",
      "Fetching up to 50 videos...\n",
      "Found 50 videos\n",
      "Fetching video details...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping channels:  24%|██▍       | 7/29 [00:03<00:10,  2.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully scraped 50 videos\n",
      "\u001b[92m[SAVED] data/CatPusic_Team.csv\n",
      "\u001b[0m\n",
      "\n",
      "============================================================\n",
      "Scraping channel: UCpLQXR116cLVUa1LRY8KS4w\n",
      "============================================================\n",
      "Channel: OwlKitty\n",
      "Subscribers: 2580000\n",
      "Total Views: 501717927\n",
      "Total Videos: 101\n",
      "\n",
      "Fetching up to 50 videos...\n",
      "Found 50 videos\n",
      "Fetching video details...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping channels:  28%|██▊       | 8/29 [00:03<00:09,  2.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully scraped 50 videos\n",
      "\u001b[92m[SAVED] data/OwlKitty.csv\n",
      "\u001b[0m\n",
      "\n",
      "============================================================\n",
      "Scraping channel: UCvmijL-eepDVHYSJHDY3d6w\n",
      "============================================================\n",
      "Channel: Cole and Marmalade\n",
      "Subscribers: 1400000\n",
      "Total Views: 444960820\n",
      "Total Videos: 477\n",
      "\n",
      "Fetching up to 50 videos...\n",
      "Found 50 videos\n",
      "Fetching video details...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping channels:  31%|███       | 9/29 [00:04<00:09,  2.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully scraped 50 videos\n",
      "\u001b[92m[SAVED] data/Cole_and_Marmalade.csv\n",
      "\u001b[0m\n",
      "\n",
      "============================================================\n",
      "Scraping channel: UCTIa8uo_aisNdqQpMf4wKTg\n",
      "============================================================\n",
      "Channel: DailyDoseOfInternetCats\n",
      "Subscribers: 1180000\n",
      "Total Views: 791577211\n",
      "Total Videos: 368\n",
      "\n",
      "Fetching up to 50 videos...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping channels:  34%|███▍      | 10/29 [00:05<00:09,  2.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 50 videos\n",
      "Fetching video details...\n",
      "Successfully scraped 50 videos\n",
      "\u001b[92m[SAVED] data/DailyDoseOfInternetCats.csv\n",
      "\u001b[0m\n",
      "\n",
      "============================================================\n",
      "Scraping channel: UCv5R_Lzpu-4PopdHiDaH7-A\n",
      "============================================================\n",
      "Channel: Chef Cat ChangAn \n",
      "Subscribers: 10700000\n",
      "Total Views: 7242246227\n",
      "Total Videos: 610\n",
      "\n",
      "Fetching up to 50 videos...\n",
      "Found 50 videos\n",
      "Fetching video details...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping channels:  38%|███▊      | 11/29 [00:05<00:08,  2.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully scraped 50 videos\n",
      "\u001b[92m[SAVED] data/Chef_Cat_ChangAn.csv\n",
      "\u001b[0m\n",
      "\n",
      "============================================================\n",
      "Scraping channel: UC4Eqt3jI2inoD4TIcvuctYQ\n",
      "============================================================\n",
      "Channel: Feeding Street Cats\n",
      "Subscribers: 1580000\n",
      "Total Views: 237969065\n",
      "Total Videos: 1731\n",
      "\n",
      "Fetching up to 50 videos...\n",
      "Found 50 videos\n",
      "Fetching video details...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping channels:  41%|████▏     | 12/29 [00:06<00:09,  1.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully scraped 50 videos\n",
      "\u001b[92m[SAVED] data/Feeding_Street_Cats.csv\n",
      "\u001b[0m\n",
      "\n",
      "============================================================\n",
      "Scraping channel: UCfVwRiihN37PIcmtlLihj6w\n",
      "============================================================\n",
      "Channel: Tasty Paws\n",
      "Subscribers: 302000\n",
      "Total Views: 71410908\n",
      "Total Videos: 390\n",
      "\n",
      "Fetching up to 50 videos...\n",
      "Found 50 videos\n",
      "Fetching video details...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping channels:  45%|████▍     | 13/29 [00:06<00:08,  1.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully scraped 50 videos\n",
      "\u001b[92m[SAVED] data/Tasty_Paws.csv\n",
      "\u001b[0m\n",
      "\n",
      "============================================================\n",
      "Scraping channel: UC-xSWdtNE9tSp9SyfPaNHkQ\n",
      "============================================================\n",
      "Channel: cat.mp4\n",
      "Subscribers: 71400\n",
      "Total Views: 26559625\n",
      "Total Videos: 58\n",
      "\n",
      "Fetching up to 50 videos...\n",
      "Found 50 videos\n",
      "Fetching video details...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping channels:  48%|████▊     | 14/29 [00:07<00:07,  1.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully scraped 50 videos\n",
      "\u001b[92m[SAVED] data/catmp4.csv\n",
      "\u001b[0m\n",
      "\n",
      "============================================================\n",
      "Scraping channel: UCuPLku1Zrk6HMr2S51yGkpQ\n",
      "============================================================\n",
      "Channel: Little Love \n",
      "Subscribers: 856000\n",
      "Total Views: 232736024\n",
      "Total Videos: 753\n",
      "\n",
      "Fetching up to 50 videos...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping channels:  52%|█████▏    | 15/29 [00:08<00:12,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 50 videos\n",
      "Fetching video details...\n",
      "Successfully scraped 50 videos\n",
      "\u001b[92m[SAVED] data/Little_Love.csv\n",
      "\u001b[0m\n",
      "\n",
      "============================================================\n",
      "Scraping channel: UCMkRHqr6MjsJ7VEhXJnSRcg\n",
      "============================================================\n",
      "Channel: Pawsome Cats of the Internet\n",
      "Subscribers: 15500\n",
      "Total Views: 13861174\n",
      "Total Videos: 77\n",
      "\n",
      "Fetching up to 50 videos...\n",
      "Found 50 videos\n",
      "Fetching video details...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping channels:  55%|█████▌    | 16/29 [00:09<00:09,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully scraped 50 videos\n",
      "\u001b[92m[SAVED] data/Pawsome_Cats_of_the_Internet.csv\n",
      "\u001b[0m\n",
      "\n",
      "============================================================\n",
      "Scraping channel: UCAUqPgpcjPk_JVv8aPYL-rw\n",
      "============================================================\n",
      "Channel: Purrfect Pets\n",
      "Subscribers: 81300\n",
      "Total Views: 20667442\n",
      "Total Videos: 50\n",
      "\n",
      "Fetching up to 50 videos...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping channels:  59%|█████▊    | 17/29 [00:09<00:07,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 50 videos\n",
      "Fetching video details...\n",
      "Successfully scraped 50 videos\n",
      "\u001b[92m[SAVED] data/Purrfect_Pets.csv\n",
      "\u001b[0m\n",
      "\n",
      "============================================================\n",
      "Scraping channel: UCU0nAvDjqfXo1fTA7xn527w\n",
      "============================================================\n",
      "Channel: CaD Animals\n",
      "Subscribers: 27900\n",
      "Total Views: 20883020\n",
      "Total Videos: 54\n",
      "\n",
      "Fetching up to 50 videos...\n",
      "Found 50 videos\n",
      "Fetching video details...\n",
      "Successfully scraped 50 videos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping channels:  62%|██████▏   | 18/29 [00:10<00:06,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m[SAVED] data/CaD_Animals.csv\n",
      "\u001b[0m\n",
      "\n",
      "============================================================\n",
      "Scraping channel: UCfm5SLKZDJqVluh-oo6q9Cw\n",
      "============================================================\n",
      "Channel: Dexter The Cat\n",
      "Subscribers: 159000\n",
      "Total Views: 131678507\n",
      "Total Videos: 203\n",
      "\n",
      "Fetching up to 50 videos...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping channels:  66%|██████▌   | 19/29 [00:10<00:05,  1.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 50 videos\n",
      "Fetching video details...\n",
      "Successfully scraped 50 videos\n",
      "\u001b[92m[SAVED] data/Dexter_The_Cat.csv\n",
      "\u001b[0m\n",
      "\n",
      "============================================================\n",
      "Scraping channel: UCezWXy_EVsFpQWQ6yk6a4FQ\n",
      "============================================================\n",
      "Channel: Renus Delph\n",
      "Subscribers: 518000\n",
      "Total Views: 171563548\n",
      "Total Videos: 370\n",
      "\n",
      "Fetching up to 50 videos...\n",
      "Found 50 videos\n",
      "Fetching video details...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping channels:  69%|██████▉   | 20/29 [00:10<00:04,  2.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully scraped 50 videos\n",
      "\u001b[92m[SAVED] data/Renus_Delph.csv\n",
      "\u001b[0m\n",
      "\n",
      "============================================================\n",
      "Scraping channel: UCW_X3-IRNdOl9vrZ-zGthlQ\n",
      "============================================================\n",
      "Channel: Funny And Cute Cat's Life\n",
      "Subscribers: 194000\n",
      "Total Views: 54318925\n",
      "Total Videos: 461\n",
      "\n",
      "Fetching up to 50 videos...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping channels:  72%|███████▏  | 21/29 [00:11<00:05,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 50 videos\n",
      "Fetching video details...\n",
      "Successfully scraped 50 videos\n",
      "\u001b[92m[SAVED] data/Funny_And_Cute_Cats_Life.csv\n",
      "\u001b[0m\n",
      "\n",
      "============================================================\n",
      "Scraping channel: UCmCOoX7E0hybWf1h7UJ4mrg\n",
      "============================================================\n",
      "Channel: Maine Coon Kittens\n",
      "Subscribers: 147000\n",
      "Total Views: 73158784\n",
      "Total Videos: 435\n",
      "\n",
      "Fetching up to 50 videos...\n",
      "Found 50 videos\n",
      "Fetching video details...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping channels:  76%|███████▌  | 22/29 [00:12<00:04,  1.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully scraped 50 videos\n",
      "\u001b[92m[SAVED] data/Maine_Coon_Kittens.csv\n",
      "\u001b[0m\n",
      "\n",
      "============================================================\n",
      "Scraping channel: UChKwZfRAjbfuWXo1NhKqHgQ\n",
      "============================================================\n",
      "Channel: Walter the Catt\n",
      "Subscribers: 704000\n",
      "Total Views: 325062818\n",
      "Total Videos: 347\n",
      "\n",
      "Fetching up to 50 videos...\n",
      "Found 50 videos\n",
      "Fetching video details...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping channels:  79%|███████▉  | 23/29 [00:13<00:03,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully scraped 50 videos\n",
      "\u001b[92m[SAVED] data/Walter_the_Catt.csv\n",
      "\u001b[0m\n",
      "\n",
      "============================================================\n",
      "Scraping channel: UC6VzUf8LyXxOz2ZkQP8_uhw\n",
      "============================================================\n",
      "Channel: CAT MAN CHRIS\n",
      "Subscribers: 938000\n",
      "Total Views: 274067260\n",
      "Total Videos: 154\n",
      "\n",
      "Fetching up to 50 videos...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping channels:  83%|████████▎ | 24/29 [00:13<00:03,  1.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 50 videos\n",
      "Fetching video details...\n",
      "Successfully scraped 50 videos\n",
      "\u001b[92m[SAVED] data/CAT_MAN_CHRIS.csv\n",
      "\u001b[0m\n",
      "\n",
      "============================================================\n",
      "Scraping channel: UCZTM8ZG29egxUmYJ_pL-iZQ\n",
      "============================================================\n",
      "Channel: TakeYourDoseOfCats\n",
      "Subscribers: 237000\n",
      "Total Views: 143080975\n",
      "Total Videos: 183\n",
      "\n",
      "Fetching up to 50 videos...\n",
      "Found 50 videos\n",
      "Fetching video details...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping channels:  86%|████████▌ | 25/29 [00:14<00:02,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully scraped 50 videos\n",
      "\u001b[92m[SAVED] data/TakeYourDoseOfCats.csv\n",
      "\u001b[0m\n",
      "\n",
      "============================================================\n",
      "Scraping channel: UCgpxaYSBRT6Ocb0wAw9S2OQ\n",
      "============================================================\n",
      "Channel: funcatflicks\n",
      "Subscribers: 28500\n",
      "Total Views: 25288964\n",
      "Total Videos: 326\n",
      "\n",
      "Fetching up to 50 videos...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping channels:  90%|████████▉ | 26/29 [00:14<00:01,  1.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 50 videos\n",
      "Fetching video details...\n",
      "Successfully scraped 50 videos\n",
      "\u001b[92m[SAVED] data/funcatflicks.csv\n",
      "\u001b[0m\n",
      "\n",
      "============================================================\n",
      "Scraping channel: UCzdQGuxoc-B9WrWQnMF4vmA\n",
      "============================================================\n",
      "Channel: FunnyPaws\n",
      "Subscribers: 173000\n",
      "Total Views: 48519956\n",
      "Total Videos: 150\n",
      "\n",
      "Fetching up to 50 videos...\n",
      "Found 50 videos\n",
      "Fetching video details...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping channels:  93%|█████████▎| 27/29 [00:14<00:00,  2.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully scraped 50 videos\n",
      "\u001b[92m[SAVED] data/FunnyPaws.csv\n",
      "\u001b[0m\n",
      "\n",
      "============================================================\n",
      "Scraping channel: UCmSu6aPS3LKmf-5MrTr3TRg\n",
      "============================================================\n",
      "Channel: Meowphorius \n",
      "Subscribers: 759000\n",
      "Total Views: 562050369\n",
      "Total Videos: 245\n",
      "\n",
      "Fetching up to 50 videos...\n",
      "Found 50 videos\n",
      "Fetching video details...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping channels:  97%|█████████▋| 28/29 [00:15<00:00,  1.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully scraped 50 videos\n",
      "\u001b[92m[SAVED] data/Meowphorius.csv\n",
      "\u001b[0m\n",
      "\n",
      "============================================================\n",
      "Scraping channel: UCxZKZ-T6Res7lrwDP9i9iUA\n",
      "============================================================\n",
      "Channel: el Cato\n",
      "Subscribers: 46600\n",
      "Total Views: 24458105\n",
      "Total Videos: 96\n",
      "\n",
      "Fetching up to 50 videos...\n",
      "Found 50 videos\n",
      "Fetching video details...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping channels: 100%|██████████| 29/29 [00:15<00:00,  1.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully scraped 50 videos\n",
      "\u001b[92m[SAVED] data/el_Cato.csv\n",
      "\u001b[0m\n",
      "\n",
      "============================================================\n",
      "\u001b[92mScraping Complete!\u001b[0m\n",
      "\u001b[94mSuccessfully scraped 29 channels\u001b[0m\n",
      "\u001b[94mCSV files saved in the 'data/' directory\u001b[0m\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Create data directory if it doesn't exist\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "# Read channel identifiers\n",
    "identifiers = read_channel_ids('accounts.txt')\n",
    "\n",
    "print_info(\"Resolving channel identifiers...\")\n",
    "channel_ids = []\n",
    "for identifier in identifiers:\n",
    "    channel_id = resolve_channel_identifier(youtube, identifier)\n",
    "    if channel_id:\n",
    "        print_success(f\"  [SUCCESS] {identifier} -> {channel_id}\")\n",
    "        channel_ids.append(channel_id)\n",
    "    else:\n",
    "        print_error(f\"  [FAILED] {identifier} -> Could not resolve\")\n",
    "\n",
    "print_info(f\"\\nResolved {len(channel_ids)} out of {len(identifiers)} channels\\n\")\n",
    "\n",
    "# Store all results\n",
    "all_channels_data = []\n",
    "channel_metadata = []\n",
    "\n",
    "# Scrape each channel\n",
    "for channel_id in tqdm(channel_ids, desc=\"Scraping channels\"):\n",
    "    df, channel_info = scrape_channel_data(youtube, channel_id, max_videos=50)\n",
    "    \n",
    "    if df is not None and channel_info is not None:\n",
    "        channel_name = channel_info['channel_title']\n",
    "        # Remove special characters and use only ASCII-safe characters\n",
    "        clean_name = ''.join(c for c in channel_name if c.isalnum() or c in (' ', '-', '_'))\n",
    "        clean_name = clean_name.replace(' ', '_').strip('_')\n",
    "        filename = f\"data/{clean_name}.csv\"\n",
    "        \n",
    "        df.to_csv(filename, index=False, encoding='utf-8')\n",
    "        print_success(f\"[SAVED] {filename}\\n\")\n",
    "        \n",
    "        all_channels_data.append(df)\n",
    "        channel_metadata.append(channel_info)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print_success(f\"Scraping Complete!\")\n",
    "print_info(f\"Successfully scraped {len(all_channels_data)} channels\")\n",
    "print_info(f\"CSV files saved in the 'data/' directory\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c925f93",
   "metadata": {},
   "source": [
    "## Summary CSV Creation\n",
    "- After scraping each channel, append its metadata to a summary list\n",
    "- At the end, convert this list to a DataFrame and save as `channels_summary.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a0d33e80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channel Summary:\n",
      "               channel_title subscriber_count video_count  total_videos_scraped   avg_views  avg_likes\n",
      "                  Crunchycat           589000         560                    50   116697.14   10485.46\n",
      "               元野良猫チャチャとR me           663000         549                    50   731521.90   21536.54\n",
      "               The Meow Show           415000          91                    50  2270376.92  151891.80\n",
      "           Princess Nika cat         10800000          80                    50 84724582.72 1682311.86\n",
      "               Chip The Manx           484000        1122                    50    20164.66    1246.86\n",
      "                     cats101            53200         203                    50    36323.78     653.48\n",
      "               CatPusic Team          1970000         320                    50   335144.12    2602.88\n",
      "                    OwlKitty          2580000         101                    50  5618675.78  156738.96\n",
      "          Cole and Marmalade          1400000         477                    50    51387.82    5173.46\n",
      "     DailyDoseOfInternetCats          1180000         368                    50   744123.96   45104.02\n",
      "           Chef Cat ChangAn          10700000         610                    50   348595.50    7104.16\n",
      "         Feeding Street Cats          1580000        1731                    50    74686.24    3393.36\n",
      "                  Tasty Paws           302000         390                    50    26023.04     332.66\n",
      "                     cat.mp4            71400          58                    50   359918.04    5932.40\n",
      "                Little Love            856000         753                    50   126751.28    1481.92\n",
      "Pawsome Cats of the Internet            15500          77                    50   162038.62    7377.30\n",
      "               Purrfect Pets            81300          50                    50   414380.18    1647.58\n",
      "                 CaD Animals            27900          54                    50   405792.84    3011.74\n",
      "              Dexter The Cat           159000         203                    50   472285.74   15370.28\n",
      "                 Renus Delph           518000         370                    50   298374.50   26548.78\n",
      "   Funny And Cute Cat's Life           194000         461                    50   138601.64    1793.10\n",
      "          Maine Coon Kittens           147000         435                    50   109141.66    3168.38\n",
      "             Walter the Catt           704000         347                    50   482568.80   27351.24\n",
      "               CAT MAN CHRIS           938000         154                    50   180943.84   10927.16\n",
      "          TakeYourDoseOfCats           237000         183                    50   193991.38   12274.80\n",
      "                funcatflicks            28500         326                    50   149897.36    5764.58\n",
      "                   FunnyPaws           173000         150                    50    44027.44    1765.74\n",
      "                Meowphorius            759000         245                    50  2823578.74   22667.52\n",
      "                     el Cato            46600          96                    50    20832.00     170.96\n",
      "\n",
      "Summary saved to: data/channels_summary.csv\n"
     ]
    }
   ],
   "source": [
    "# Create summary DataFrame from channel metadata\n",
    "summary_df = pd.DataFrame(channel_metadata)\n",
    "\n",
    "# Calculate average engagement per channel from video data\n",
    "# Match up with channel metadata by index (they're in the same order)\n",
    "engagement_stats = []\n",
    "for i, df in enumerate(all_channels_data):\n",
    "    if len(df) > 0:\n",
    "        # Get the corresponding channel info\n",
    "        channel_info = channel_metadata[i]\n",
    "        \n",
    "        stats = {\n",
    "            'channel_id': channel_info['channel_id'],\n",
    "            'channel_title': channel_info['channel_title'],\n",
    "            'total_videos_scraped': len(df),\n",
    "            'avg_views': df['view_count'].mean(),\n",
    "            'avg_likes': df['like_count'].mean(),\n",
    "            'avg_comments': df['comment_count'].mean(),\n",
    "            'total_views_scraped_videos': df['view_count'].sum(),\n",
    "            'total_likes_scraped_videos': df['like_count'].sum(),\n",
    "        }\n",
    "        engagement_stats.append(stats)\n",
    "\n",
    "engagement_df = pd.DataFrame(engagement_stats)\n",
    "\n",
    "# Merge with channel metadata\n",
    "if len(engagement_df) > 0:\n",
    "    summary_full = pd.merge(summary_df, engagement_df, on=['channel_id', 'channel_title'], how='left')\n",
    "    \n",
    "    # Save summary\n",
    "    summary_full.to_csv('data/channels_summary.csv', index=False)\n",
    "    print(\"Channel Summary:\")\n",
    "    print(summary_full[['channel_title', 'subscriber_count', 'video_count', \n",
    "                        'total_videos_scraped', 'avg_views', 'avg_likes']].to_string(index=False))\n",
    "    print(f\"\\nSummary saved to: data/channels_summary.csv\")\n",
    "else:\n",
    "    print(\"No data scraped\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
